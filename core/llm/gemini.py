import google.generativeai as genai
from langchain.llms.base import LLM
from typing import Any, List, Optional, Generator
from langchain.callbacks.manager import CallbackManagerForLLMRun
from pydantic import Field
import logging
import time

logger = logging.getLogger("enhanced_medical_chatbot")

class GeminiLLM(LLM):
    # Khai b√°o field cho pydantic
    model_name: str = Field(default="gemini-1.5-flash", description="T√™n model Gemini")

    @property
    def _llm_type(self) -> str:
        return "gemini"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        try:
            model = genai.GenerativeModel(self.model_name)
            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=4000,  # ‚úÖ TƒÇNG L√äN ƒê·ªÇ CHO PH√âP PH·∫¢N H·ªíI D√ÄI
                    temperature=0.7,
                    top_p=0.8,
                    top_k=40,
                ),
                safety_settings=[
                    # ‚úÖ GI·∫¢M ƒê·ªò NGHI√äM NG·∫∂T ƒê·ªÇ TR√ÅNH BLOCK PH·∫¢N H·ªíI Y T·∫æ
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_ONLY_HIGH"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"},
                ]
            )
            
            if hasattr(response, 'text') and response.text:
                return response.text
            else:
                logger.warning("No text in Gemini response")
                return "Xin l·ªói, kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi. Vui l√≤ng th·ª≠ l·∫°i."
                
        except Exception as e:
            logger.error(f"Gemini API error: {e}")
            error_msg = str(e).lower()
            
            # ‚úÖ X·ª¨ L√ù C√ÅC LO·∫†I L·ªñI C·ª§ TH·ªÇ
            if "safety" in error_msg:
                return "Xin l·ªói, n·ªôi dung ph·∫£n h·ªìi b·ªã gi·ªõi h·∫°n do ch√≠nh s√°ch an to√†n. Vui l√≤ng h·ªèi c√¢u kh√°c."
            elif "quota" in error_msg or "rate" in error_msg:
                return "H·ªá th·ªëng ƒëang qu√° t·∫£i. Vui l√≤ng th·ª≠ l·∫°i sau √≠t ph√∫t."
            elif "timeout" in error_msg:
                return "K·∫øt n·ªëi timeout. Vui l√≤ng th·ª≠ l·∫°i."
            else:
                return f"Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t: {str(e)}"


class GeminiStreamingLLM(GeminiLLM):
    def stream_response(self, prompt: str, max_retries: int = 3) -> Generator[str, None, None]:
        """
        Stream response v·ªõi retry mechanism v√† error handling t·ªët h∆°n.
        """
        for attempt in range(max_retries):
            try:
                logger.info(f"Starting Gemini stream attempt {attempt + 1}/{max_retries}")
                
                model = genai.GenerativeModel(self.model_name)
                response = model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        max_output_tokens=4000,  # ‚úÖ TƒÇNG ƒê·ªÇ CHO PH√âP PH·∫¢N H·ªíI D√ÄI
                        temperature=0.7,
                        top_p=0.8,
                        top_k=40,
                        # ‚úÖ KH√îNG SET STOP SEQUENCES ƒê·ªÇ TR√ÅNH D·ª™NG S·ªöM
                    ),
                    stream=True,
                    safety_settings=[
                        # ‚úÖ GI·∫¢M ƒê·ªò NGHI√äM NG·∫∂T
                        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
                        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
                        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_ONLY_HIGH"},
                        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"},
                    ]
                )
                
                chunk_count = 0
                total_text = ""
                start_time = time.time()
                last_chunk_time = start_time
                
                # ‚úÖ C·∫¢I THI·ªÜN CHUNK PROCESSING
                for chunk in response:
                    try:
                        current_time = time.time()
                        
                        # ‚úÖ TIMEOUT PROTECTION
                        if current_time - last_chunk_time > 30:  # 30 gi√¢y kh√¥ng c√≥ chunk m·ªõi
                            logger.warning("No new chunks for 30 seconds, stopping")
                            if chunk_count == 0:
                                raise TimeoutError("No chunks received")
                            break
                        
                        chunk_text = ""
                        
                        # ‚úÖ X·ª¨ L√ù CHUNK LINH HO·∫†T H·ª†N
                        if hasattr(chunk, 'text') and chunk.text:
                            chunk_text = chunk.text
                        elif hasattr(chunk, 'parts') and chunk.parts:
                            for part in chunk.parts:
                                if hasattr(part, 'text') and part.text:
                                    chunk_text += part.text
                        
                        if chunk_text:
                            chunk_count += 1
                            total_text += chunk_text
                            last_chunk_time = current_time
                            yield chunk_text
                        
                        # ‚úÖ CHECK FINISH REASON
                        if hasattr(chunk, 'candidates'):
                            for candidate in chunk.candidates:
                                if hasattr(candidate, 'finish_reason') and candidate.finish_reason:
                                    reason = candidate.finish_reason
                                    if hasattr(reason, 'name'):
                                        reason_name = reason.name
                                        logger.info(f"Stream finished with reason: {reason_name}")
                                        
                                        if reason_name == 'MAX_TOKENS':
                                            yield "\n\nüìù *[Ph·∫£n h·ªìi ƒë√£ ƒë·∫°t gi·ªõi h·∫°n ƒë·ªô d√†i]*"
                                        elif reason_name in ['SAFETY', 'RECITATION']:
                                            yield f"\n\n‚ö†Ô∏è *[Ph·∫£n h·ªìi b·ªã gi·ªõi h·∫°n: {reason_name}]*"
                                        
                                        # ‚úÖ RETURN CH·ªà KHI TH·ª∞C S·ª∞ FINISHED
                                        if reason_name in ['STOP', 'MAX_TOKENS', 'SAFETY', 'RECITATION']:
                                            logger.info(f"Streaming completed: {chunk_count} chunks, {len(total_text)} chars")
                                            return
                        
                    except Exception as chunk_error:
                        logger.error(f"Chunk processing error: {chunk_error}")
                        continue
                
                # ‚úÖ N·∫æU H·∫æT CHUNKS NH∆ØNG KH√îNG C√ì FINISH REASON
                if chunk_count > 0:
                    logger.info(f"Streaming completed naturally: {chunk_count} chunks, {len(total_text)} chars")
                    return
                else:
                    raise ValueError("No chunks received")
                    
            except Exception as e:
                error_msg = str(e).lower()
                logger.error(f"Streaming attempt {attempt + 1} failed: {e}")
                
                # ‚úÖ N·∫æU L√Ä L·∫¶N TH·ª¨ CU·ªêI
                if attempt == max_retries - 1:
                    if "safety" in error_msg:
                        yield "‚ö†Ô∏è Ph·∫£n h·ªìi b·ªã gi·ªõi h·∫°n do ch√≠nh s√°ch an to√†n. Vui l√≤ng th·ª≠ c√¢u h·ªèi kh√°c."
                    elif "quota" in error_msg or "rate" in error_msg:
                        yield "‚ö†Ô∏è H·ªá th·ªëng ƒëang qu√° t·∫£i. Vui l√≤ng th·ª≠ l·∫°i sau √≠t ph√∫t."
                    elif "timeout" in error_msg:
                        yield "‚ö†Ô∏è K·∫øt n·ªëi timeout. Vui l√≤ng th·ª≠ l·∫°i."
                    else:
                        yield f"‚ùå L·ªói k·ªπ thu·∫≠t: {str(e)}"
                    return
                else:
                    # ‚úÖ TH·ª¨ L·∫†I
                    yield f"üîÑ L·ªói k·∫øt n·ªëi, ƒëang th·ª≠ l·∫°i l·∫ßn {attempt + 2}...\n\n"
                    time.sleep(2)  # ƒê·ª£i tr∆∞·ªõc khi retry

    def test_connection(self) -> dict:
        """Test k·∫øt n·ªëi v·ªõi Gemini API."""
        try:
            model = genai.GenerativeModel(self.model_name)
            test_response = model.generate_content(
                "Ch√†o b·∫°n!",
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=50,
                    temperature=0.7,
                )
            )
            
            return {
                "status": "success",
                "model": self.model_name,
                "response": test_response.text if hasattr(test_response, 'text') else "No response",
                "response_length": len(test_response.text) if hasattr(test_response, 'text') else 0
            }
            
        except Exception as e:
            return {
                "status": "error",
                "model": self.model_name,
                "error": str(e)
            }