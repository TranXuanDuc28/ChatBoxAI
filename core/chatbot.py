from core.llm.gemini import GeminiLLM, GeminiStreamingLLM
from core.classifier.medical_classifier import MedicalClassifier
from core.vector_store.enhanced_vector_store import EnhancedVectorStore
from core.vector_store.embeddings import SentenceTransformerEmbeddings
from core.data.web_data_manager import WebDataManager
from config.prompts import MEDICAL_SYSTEM_PROMPT
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_core.prompts import PromptTemplate
import logging
from config.settings import GEMINI_API_KEY

logger = logging.getLogger("enhanced_medical_chatbot")

class EnhancedMedicalChatbot:
    def __init__(self):
        self.medical_classifier = MedicalClassifier()
        self.web_data_manager = WebDataManager()
        self.embeddings = SentenceTransformerEmbeddings()
        self.vector_store = EnhancedVectorStore(self.embeddings)
        self.llm = GeminiLLM()
        self.streaming_llm = GeminiStreamingLLM()
        self.memory = ConversationBufferWindowMemory(
            k=6,
            memory_key="chat_history",
            return_messages=True
        )
        self.rag_chain = None
        self.suggestion_chain = None
        self.setup_chains()
    
    def setup_chains(self):
        web_data = self.web_data_manager.get_web_data()
        self.vector_store.build_from_web_data(web_data)
        
        if self.vector_store.vectorstore is not None:
            retriever = self.vector_store.as_retriever(search_kwargs={"k": 7})
            prompt_template = PromptTemplate(
                input_variables=["context", "chat_history", "question"],
                template=MEDICAL_SYSTEM_PROMPT
            )
            self.rag_chain = ConversationalRetrievalChain.from_llm(
                llm=self.llm,
                retriever=retriever,
                memory=self.memory,
                return_source_documents=True,
                combine_docs_chain_kwargs={"prompt": prompt_template}
            )
            logger.info("RAG chain initialized successfully")
        else:
            logger.warning("Vector store not available, using simple LLM chain")
            prompt_template = PromptTemplate(
                input_variables=["chat_history", "question"],
                template="""B·∫°n l√† tr·ª£ l√Ω y t·∫ø. 
                Chat history: {chat_history}
                Human: {question}
                Assistant:"""
            )
            # ‚úÖ D√πng pipe thay LLMChain
            self.rag_chain = prompt_template | self.llm
        
        # ‚úÖ Suggestion chain thay th·∫ø LLMChain
        suggestion_prompt = PromptTemplate(
            input_variables=["original_question"],
            template="""Ng∆∞·ªùi d√πng v·ª´a h·ªèi: "{original_question}"
Tuy nhi√™n c√¢u h·ªèi n√†y kh√¥ng thu·ªôc lƒ©nh v·ª±c y t·∫ø.
H√£y ƒë·ªÅ xu·∫•t 1 c√¢u h·ªèi t∆∞∆°ng t·ª± nh∆∞ng li√™n quan ƒë·∫øn y t·∫ø.
Tr·∫£ l·ªùi b·∫±ng 1 c√¢u h·ªèi duy nh·∫•t, ti·∫øng Vi·ªát:"""
        )
        self.suggestion_chain = suggestion_prompt | self.llm
    
    def refresh_data(self):
        logger.info("Refreshing web data...")
        self.web_data_manager.last_update = 0
        self.setup_chains()
    def get_health_status(self):
        """Get system health status"""
        return {
            "status": "healthy",
            "model": "gemini-1.5-flash",
            "classifier_loaded": self.medical_classifier.model is not None,
            "vector_store_ready": self.vector_store.vectorstore is not None,
            "rag_chain_ready": self.rag_chain is not None,
            "memory_messages": len(self.memory.chat_memory.messages),
            "api_key_configured": bool(GEMINI_API_KEY)
        }
    
    def generate_response(self, user_message: str) -> str:
        try:
            if self.rag_chain is None:
                return "H·ªá th·ªëng ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. Vui l√≤ng th·ª≠ l·∫°i."
            
            is_medical, confidence = self.medical_classifier.is_medical_question(user_message)
            if not is_medical and confidence > 0.7:
                try:
                    # ‚úÖ invoke thay v√¨ run
                    suggestion = self.suggestion_chain.invoke({"original_question": user_message})
                    # N·∫øu output l√† dict th√¨ l·∫•y key 'text'
                    if isinstance(suggestion, dict) and "text" in suggestion:
                        suggestion = suggestion["text"]
                    return f"‚ùå T√¥i ch·ªâ h·ªó tr·ª£ tr·∫£ l·ªùi c√°c c√¢u h·ªèi trong lƒ©nh v·ª±c **y t·∫ø**.\n\nüí° G·ª£i √Ω: {suggestion}"
                except Exception as e:
                    logger.error(f"Suggestion generation error: {e}")
                    return "‚ùå T√¥i ch·ªâ h·ªó tr·ª£ tr·∫£ l·ªùi c√°c c√¢u h·ªèi trong lƒ©nh v·ª±c **y t·∫ø**.\n\nüí° H√£y th·ª≠ h·ªèi v·ªÅ s·ª©c kh·ªèe, tri·ªáu ch·ª©ng b·ªánh ho·∫∑c thu·ªëc nh√©."
            
            # ‚úÖ chain c√≥ 2 lo·∫°i: ConversationalRetrievalChain ho·∫∑c prompt|llm
            if hasattr(self.rag_chain, "invoke"):
                result = self.rag_chain.invoke({
                    "chat_history": self.memory.chat_memory.messages,
                    "question": user_message
                })
                if isinstance(result, dict) and "answer" in result:
                    return result["answer"]
                elif isinstance(result, str):
                    return result
                else:
                    return str(result)
            else:
                return "Kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi t·ª´ chain."
                
        except Exception as e:
            logger.error(f"Response generation error: {e}")
            return "Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë. Vui l√≤ng th·ª≠ l·∫°i sau."
    def generate_streaming_response(self, user_message: str):
        try:
            is_medical, confidence = self.medical_classifier.is_medical_question(user_message)
            if not is_medical and confidence > 0.7:
                try:
                    suggestion = self.suggestion_chain.invoke({"original_question": user_message})
                    if isinstance(suggestion, dict) and "text" in suggestion:
                        suggestion = suggestion["text"]
                    error_msg = f"‚ùå T√¥i ch·ªâ h·ªó tr·ª£ tr·∫£ l·ªùi c√°c c√¢u h·ªèi trong lƒ©nh v·ª±c **y t·∫ø**.\n\nüí° G·ª£i √Ω: {suggestion}"
                except Exception:
                    error_msg = "‚ùå T√¥i ch·ªâ h·ªó tr·ª£ tr·∫£ l·ªùi c√°c c√¢u h·ªèi trong lƒ©nh v·ª±c **y t·∫ø**.\n\nüí° H√£y th·ª≠ h·ªèi v·ªÅ s·ª©c kh·ªèe, tri·ªáu ch·ª©ng b·ªánh ho·∫∑c thu·ªëc nh√©."
                yield error_msg
                return
            
            # üîπ L·∫•y context t·ª´ vectorstore
            context = ""
            if self.vector_store.vectorstore is not None:
                try:
                    retriever = self.vector_store.as_retriever(search_kwargs={"k": 5})
                    docs = retriever.get_relevant_documents(user_message)
                   
                    context = "\n".join([doc.page_content for doc in docs])
                    print("context", context)
                except Exception as e:
                    logger.warning(f"Context retrieval error: {e}")
            
            # üîπ T·∫°o chat_history text
            chat_history = ""
            if self.memory.chat_memory.messages:
                recent_messages = self.memory.chat_memory.messages[-4:]
                for msg in recent_messages:
                    if hasattr(msg, 'content'):
                        role = "Human" if msg.__class__.__name__ == "HumanMessage" else "Assistant"
                        chat_history += f"{role}: {msg.content}\n"
            
            # üîπ T·∫°o prompt ƒë·∫ßy ƒë·ªß
            prompt = MEDICAL_SYSTEM_PROMPT.format(
                context=context if context else "Kh√¥ng c√≥ th√¥ng tin b·ªï sung",
                chat_history=chat_history,
                question=user_message
            )
            print("prompt", prompt)
            
            # üîπ Stream response t·ª´ Gemini
            full_response = ""
            for chunk in self.streaming_llm.stream_response(prompt):
                full_response += chunk
                yield chunk
            
            # üîπ C·∫≠p nh·∫≠t memory
            try:
                self.memory.chat_memory.add_user_message(user_message)
                self.memory.chat_memory.add_ai_message(full_response)
            except Exception as e:
                logger.warning(f"Memory update error: {e}")
                
        except Exception as e:
            logger.error(f"Streaming error: {e}")
            yield "Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë. Vui l√≤ng th·ª≠ l·∫°i sau."

enhanced_chatbot = EnhancedMedicalChatbot()